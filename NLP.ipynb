{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief explanation of the dataset & features\n",
    "\n",
    "Consumer Complaint Narrative: This is a paragraph (or text) written by the customer explaining his complaint in detail. The data is a string type consisting of text in the form of paragraphs.\n",
    "Product: This is the category we are to classify each complaint to. The 12 categories the complaints need to be categorized into are:\n",
    "\n",
    "'Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service', 'Prepaid card'\n",
    "\n",
    "<h3>What we want as the outcome?</h3>\n",
    "\n",
    "We would classify each complaint to its respective category, so that the complaint can be directed to the right vertical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\snake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\snake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('wordnet')\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>When my loan was switched over to Navient i wa...</td>\n",
       "      <td>Student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I tried to sign up for a spending monitoring p...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0                                                NaN   \n",
       "1  When my loan was switched over to Navient i wa...   \n",
       "2  I tried to sign up for a spending monitoring p...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                             y  \n",
       "0                     Mortgage  \n",
       "1                 Student loan  \n",
       "2  Credit card or prepaid card  \n",
       "3                  Credit card  \n",
       "4              Debt collection  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading of dataset\n",
    "full_data = pd.read_csv('file.csv')\n",
    "# keeping the relevant columns\n",
    "data = full_data[[\"Consumer complaint narrative\",\"Product\"]]\n",
    "data.columns = ['X','y']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is it difficult to work with text?\n",
    "Comprehending Language is hard for computers. Some of the unique challenges of working with text are as follows:\n",
    "\n",
    "Synonymy - This corresponds to different words having the same meaning. A similar intent can be conveyed in various ways and this is one of the prime reasons, why computers have a hard time deciphering the meaning or intent of those statements. \"The President of United States has signed a new decree\" and \"POTUS has inked in a new law\" are basically advocating the same sentiment. However as they are completely different sentences syntactically, computers have a hard time figuring out the user intent.\n",
    "\n",
    "Ambiguity - \"The bank deposit rate is quite high\" and \"He stood near the bank admiring the river\". In these statements, the word bank has completely different meanings. In the first case it represents a financial institution, and in the second case it refers to land near the river. Disambiguating the meaning in sentences is quite challenging.\n",
    "\n",
    "Anaphora Resolution - \"George is my friend. He likes football\". In the second statement he refers to George. It is difficult for the computers to discern what person/entity the pronoun he is referring to.\n",
    "\n",
    "Language related issues - Every language has its own uniqueness. For English we have words, sentences, paragraphs and so on. But in Thai, there is no concept of sentences at all! The grammar and morphology of languages is so different. This is why we observe that Google Translator or any other translator service struggles to perfectly convert a piece of text from one language to another.\n",
    "\n",
    "Out of Vocabulary problem - Machines have a hard time adapting to any new constructs that humans come up with. As humans when we come across a word we haven't seen earlier, we might not understand its meaning instantly. But this does not mean we cannot adapt. After looking at the word in several different sentences and understanding its usage, we understand the context and meaning of the new word. Machines can only handle data that they have seen before. It is unable to adapt well.\n",
    "\n",
    "Language generation - While language understanding is hard, language generation too has its own set of challenges. For chatbots to work effectively, they need to communication properly constructed sentences which are grammatically correct. This is quite a hard problem and a challenge that needs to be overcome.\n",
    "\n",
    "We now know that working with text is hard. But there are also exciting applications and use cases involved with working on text. We will now take a look at some of the use cases.\n",
    "\n",
    "<h2>Usecases of NLP</h2>\n",
    "The usecases of NLP encompass almost anything you can do with Language in relation to a problem.\n",
    "\n",
    "1) Sentiment Analysis - Finding if the text is leaning towards a positive or negative sentiment.\n",
    "\n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral is called Sentiment Analysis. The information present over the Internet is constantly growing resulting in a large number of texts expressing opinions in review sites, forums, blogs and different social media forums. Sentiment analysis is therefore a topic of great interest and development since it has many practical applications. It is immensely useful in figuring the overall sentiment of products (Amazon), movies (Netflix), food (Yelp),etc. Its applications include Market Research, Social Monitoring, Customer Support and Product Analytics.\n",
    "\n",
    "2) Text Classification - Categorizing text to various categories\n",
    "\n",
    "Text classifiers can be used to organize, structure, and categorize almost any text data we have. For e.g. New articles can be organized by topics, chat conversations can be organized by language, support tickets can be organized by urgency etc. Other examples of text classification include:\n",
    "\n",
    "Directing customer queries to the right vertical\n",
    "\n",
    "Detection of spam and non-spam emails,\n",
    "\n",
    "Auto tagging of customer queries\n",
    "\n",
    "3) Document Summarization - Compressing a paragraph/document into few words or sentences\n",
    "\n",
    "Text summarization is the method of compressing a text document, in order to create a summary of the major points of the document. The idea of summarization is to find a subset of data which contains the information of the entire set. It's applications include News summary(Inshorts app), Novel Summary, Book Summary (Blinkist) etc. With the overall attention span declining, the need to provide information in the shortest possible words has risen - and summarization helps solve this problem.\n",
    "\n",
    "4) Parts of Speech Tagging - Figuring out the various nouns, adverbs, verbs etc in the text\n",
    "\n",
    "Identifying part of speech tags is much more complicated than it looks. This is because over time in the development of language, a single word can have different parts of speech tag in different sentences based on different contexts. This makes it impossible to have a generic mapping for POS tags. Few of its applications include:\n",
    "\n",
    "Text to speech conversion\n",
    "\n",
    "Word Sense Disambiguation (Teach machine to know the difference of the meaning of word 'bears' in \"I saw a couple of bears\" and \"Hard work always bears fruit\")\n",
    "\n",
    "5) Machine translation - Translate text from one language to another\n",
    "\n",
    "Machine Translation is the task of automatically translating one natural language into another while retaining the meaning of the original text. Translation from one language to another is complex because some of the words in the original language could have multiple meanings and these words could have different forms in the output language. Its most popular application is Google Translate and it is employed in devices like Google Home as well. Machine translation allows business transactions between partners in different countries without the need of a human interpreter.\n",
    "\n",
    "6) Named Entity Recognition - Identify the entities present in text\n",
    "\n",
    "Named Entity Recognition deals with named entity mentions in text and categorizes these entities into person, organization, datetime reference etc. This is used a lot in the field of bioinformatics, molecular biology and other medical NLP applications. It also plays an important role in the overall field of Information Extraction where we try to extract knowledge from unstructured text.\n",
    "\n",
    "7) Conversational AI - Chat with a machine in natural language and get queries resolved\n",
    "\n",
    "Conversational AI deals with creating an interface between machines and humans to converse in natural language. Such interfaces are known as chatbots. A user can interact in natural language with natural language, the same way he usually communicates with a human. For organizations to truly scale in terms of customer support, chatbots are increasingly adopted as the first point of contact for customer query resolution across all organizations.\n",
    "\n",
    "So for enabling all the NLP usecases, the first challenge is to convert the text into a form that the machine can understand. For that, we need to arrive at a fundamental component of text known as tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Motivation for tokenization</h3>\n",
    "\n",
    "We can see that unlike all the machine learning datasets we have worked with previously, the data isn't boolean, numeric, categorical etc. Usually a text is composed of paragraphs, paragraphs are composed of sentences, and sentences are composed of words. You could also go deeper into letters, but the letters have no meaning. It's only when they are combined into words, that the text starts to make sense. Hence, it is better to work at the word level.\n",
    "\n",
    "Tokenization is the process of splitting the text into smaller parts called tokens. Tokens are the basic units of a particular dataset. The choice of tokens could be based on the application we are working on.\n",
    "\n",
    "<h3>Introduction to NLTK</h3>\n",
    "\n",
    "Natural Language Tool Kit/NLTK is the standard library in python which specifically deals with text. All the text processing tasks could be easily done with this library. It is a leading platform for building Python programs to work with human language data. It also provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, along with an active discussion forum. On top of it, it is completely free and open-source with a vibrant developer community supporting it. Let us now take the first step towards categorizing the consumer complaints by starting with tokenization.\n",
    "\n",
    "<h3>Tokenizing with NLTK - The problem intuition</h3>\n",
    "\n",
    "We will first need to find a way to convert the text to numbers to get them to a form where you would be able to apply an algorithm to this. Think of this like sklearn, which require all non-numeric data to be encoded (label or one-hot) prior to the sklearn pipeline.\n",
    "\n",
    "Intuitively, it would make sense to divide each paragraph of text to its basic form (words) and then convert each of those words to numbers. We could assign a particular number to each word, in which case a sentence could look like a set of numbers to us, each number representing a particular word.\n",
    "\n",
    "The first step to achieving that would be to break the text down to words. That's what tokenization aims to do. NLTK has a built in libraries for tokenization which we will use for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Complaint\n",
      "\n",
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Using the Split Command\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up.']\n",
      "\n",
      "Using tokenize\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "# Dropping nan values from dataframe\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "\n",
    "# Printing the first complaint\n",
    "print(\"\\nFirst Complaint\\n\")\n",
    "print(first_complaint)\n",
    "\n",
    "# Using the split command\n",
    "print(\"\\nUsing the Split Command\\n\")\n",
    "bag_of_words_1 = first_complaint.split(\" \")\n",
    "print(bag_of_words_1)\n",
    "\n",
    "# Using the tokenize command\n",
    "print(\"\\nUsing tokenize\\n\")\n",
    "bag_of_words_2 = word_tokenize(first_complaint)\n",
    "print(bag_of_words_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sentences\n",
      " ['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not.', 'When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX.', 'I have been faithful at paying my student loan.', 'I was told that Navient was the company i had delinquency with.', 'I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me.', 'I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus.', 'I have had so much trouble bringing my credit score back up.']\n",
      "\n",
      " ['when', 'my', 'loan', 'was', 'switched', 'over', 'to', 'navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'xxxx', 'i', 'did', 'not', '.', 'when', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'xxxx', 'into', 'the', 'xxxx', '.', 'i', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'i', 'was', 'told', 'that', 'navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'i', 'contacted', 'navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'i', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'i', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "# first_complaint is already loaded onto the workspace\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenizing sentences\n",
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "\n",
    "print(\"List of sentences\\n\", list_of_sentences)\n",
    "\n",
    "# Lowering first complaint\n",
    "first_complaint_lower = first_complaint.lower()\n",
    "\n",
    "# Tokenizing first complaint lower\n",
    "bag_of_words_lower = word_tokenize(first_complaint_lower)\n",
    "\n",
    "print(\"\\n\",bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of converting the words of a sentence to its non-changing portions. So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "For eg: Likes, liked, likely, unlike\\Rightarrowâ‡’like\n",
    "\n",
    "Lot of different algorithms have been defined for the process, each with their own set of rules. The popular ones include:\n",
    "\n",
    "Porter Stemmer(Implemented in almost all languages)\n",
    "\n",
    "Paice Stemmer\n",
    "\n",
    "Lovins Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words of text: Natural Language Processing is really fun and I want to study it more \n",
      "is stemmed in the following way: \n",
      "['natur', 'languag', 'process', 'is', 'realli', 'fun', 'and', 'I', 'want', 'to', 'studi', 'it', 'more']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text=\"Natural Language Processing is really fun and I want to study it more\"\n",
    "print(\"The words of text:\",text,\"\\nis stemmed in the following way: \")\n",
    "\n",
    "#Breaking the sentence to words\n",
    "tokens=text.split()\n",
    "\n",
    "#Defining Porter Stemmer object\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "#Applying the stemming\n",
    "stem = [porter.stem(i) for i in tokens]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization:\n",
    "\n",
    "This method is a more refined way of breaking words through the use of a vocabulary and morphological analysis of words. The aim is to always return the base form of a word known as lemma.\n",
    "\n",
    "Consider the following words:\n",
    "\n",
    "'Studied', 'Studious' ,'Studying'\n",
    "\n",
    "Stemming of them will result in Studi\n",
    "\n",
    "Lemmatisation of them will result in Study\n",
    "\n",
    "As it can be seen Lemmatization is more complex than stemming because it requires words to be categorized by a part-of-speech as well as by inflected form.\n",
    "\n",
    "In languages other than English, it can become quite complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words of text: Women in  technology are amazing at coding \n",
      "is lemmatized in the following way: \n",
      "['woman', 'in', 'technology', 'are', 'amazing', 'at', 'coding']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "text = \"Women in  technology are amazing at coding\"\n",
    "print(\"The words of text:\",text,\"\\nis lemmatized in the following way: \")\n",
    "\n",
    "tokens=text.lower().split()\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma_result = [lemma.lemmatize(i) for i in tokens]\n",
    "print(lemma_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Bag of words:\n",
    "The problem with modeling text is that there is no well defined fixed-length inputs.\n",
    "\n",
    "A bag of words model is a way of extracting features from text for use in modeling. In this approach, we use the tokenized words for each observation and find out the frequency of each token.\n",
    "\n",
    "Let's take an example to understand it.\n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "\"Hope is a good thing\"\n",
    "\"Maybe the best thing\"\n",
    "\"No good thing ever dies\"\n",
    "We will treat each sentence as a different document and make a list of all unique words from the three documentations. We get:\n",
    "\n",
    "\"hope\", \"is\", \"a\", \"good\", \"thing\", \"maybe\", \"the\", \"best\", \"no\", \"ever\", \"dies\"\n",
    "\n",
    "Next, we try to create vectors from it.\n",
    "\n",
    "In this, we take the first document = \"Hope is a good thing\" and check the frequency of words from the 11 unique words:\n",
    "\n",
    "\"hope\" - 1\n",
    "\" is\" - 1\n",
    "\"a\" - 1\n",
    "\"good\" - 1\n",
    "\"thing\" - 1\n",
    "\"maybe\" - 0\n",
    "\"the\"-0\n",
    "\"best\" - 0\n",
    "\"no\" - 0\n",
    "\"ever\" - 0\n",
    "\"dies\" - 0\n",
    "Following is how each document will look like:\n",
    "\n",
    "\"Hope is a good thing\" - [1,1,1,1,1,0,0,0,0,0,0]\n",
    "\n",
    "\"Maybe the best thing\" - [0,0,0,0,1,1,1,1,0,0,0]\n",
    "\n",
    "\"No good thing ever dies\" - [0,0,0,1,1,0,0,0,0,1,1]\n",
    "\n",
    "This process of converting text data to numbers is called vectorization\n",
    "\n",
    "There are multiple methods to convert words to numbers. We will be start with discussing the count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'when': 2,\n",
       "         'my': 4,\n",
       "         'loan': 2,\n",
       "         'was': 5,\n",
       "         'switched': 1,\n",
       "         'over': 1,\n",
       "         'to': 5,\n",
       "         'navient': 3,\n",
       "         'i': 11,\n",
       "         'never': 1,\n",
       "         'told': 3,\n",
       "         'that': 3,\n",
       "         'had': 4,\n",
       "         'a': 2,\n",
       "         'deliquint': 1,\n",
       "         'balance': 2,\n",
       "         'because': 1,\n",
       "         'with': 3,\n",
       "         'xxxx': 3,\n",
       "         'did': 1,\n",
       "         'not': 1,\n",
       "         '.': 7,\n",
       "         'going': 1,\n",
       "         'purchase': 1,\n",
       "         'vehicle': 1,\n",
       "         'discovered': 1,\n",
       "         'credit': 4,\n",
       "         'score': 2,\n",
       "         'been': 2,\n",
       "         'dropped': 1,\n",
       "         'from': 1,\n",
       "         'the': 8,\n",
       "         'into': 1,\n",
       "         'have': 2,\n",
       "         'faithful': 1,\n",
       "         'at': 1,\n",
       "         'paying': 1,\n",
       "         'student': 1,\n",
       "         'company': 1,\n",
       "         'delinquency': 2,\n",
       "         'contacted': 1,\n",
       "         'resolve': 1,\n",
       "         'this': 1,\n",
       "         'issue': 1,\n",
       "         'you': 1,\n",
       "         'and': 5,\n",
       "         'kept': 1,\n",
       "         'being': 1,\n",
       "         'just': 2,\n",
       "         'contact': 1,\n",
       "         'bureaus': 2,\n",
       "         'expalin': 1,\n",
       "         'situation': 1,\n",
       "         'maybe': 1,\n",
       "         'they': 1,\n",
       "         'could': 1,\n",
       "         'help': 1,\n",
       "         'me': 1,\n",
       "         'so': 2,\n",
       "         'angry': 1,\n",
       "         'hurried': 1,\n",
       "         'paid': 1,\n",
       "         'off': 1,\n",
       "         'then': 1,\n",
       "         'after': 1,\n",
       "         'tried': 1,\n",
       "         'dispute': 1,\n",
       "         'much': 1,\n",
       "         'trouble': 1,\n",
       "         'bringing': 1,\n",
       "         'back': 1,\n",
       "         'up': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count_vectorizer = Counter(bag_of_words_lower)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Row:\n",
      " ['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.']\n",
      "\n",
      "Vector Shape:\n",
      " (1, 69)\n",
      "\n",
      "Vector Values:\n",
      " [[1 5 1 1 1 2 1 2 1 1 2 1 1 1 1 4 2 1 1 1 1 1 1 1 1 1 4 2 1 1 1 1 2 1 2 1\n",
      "  1 1 4 3 1 1 1 1 1 1 1 1 2 1 2 1 1 3 8 1 1 1 5 3 1 1 1 1 5 2 3 3 1]]\n",
      "These are the counts of the 69 unique words in our first complaint.\n",
      "\n",
      "Count Vectorizer Vocabulary:\n",
      " {'when': 65, 'my': 38, 'loan': 34, 'was': 64, 'switched': 52, 'over': 43, 'to': 58, 'navient': 39, 'never': 40, 'told': 59, 'that': 53, 'had': 26, 'deliquint': 17, 'balance': 5, 'because': 6, 'with': 66, 'xxxx': 67, 'did': 18, 'not': 41, 'going': 25, 'purchase': 46, 'vehicle': 63, 'discovered': 19, 'credit': 15, 'score': 48, 'been': 7, 'dropped': 21, 'from': 24, 'the': 54, 'into': 30, 'have': 27, 'faithful': 23, 'at': 3, 'paying': 45, 'student': 51, 'company': 11, 'delinquency': 16, 'contacted': 13, 'resolve': 47, 'this': 57, 'issue': 31, 'you': 68, 'and': 1, 'kept': 33, 'being': 8, 'just': 32, 'contact': 12, 'bureaus': 10, 'expalin': 22, 'situation': 49, 'maybe': 35, 'they': 56, 'could': 14, 'help': 28, 'me': 36, 'so': 50, 'angry': 2, 'hurried': 29, 'paid': 44, 'off': 42, 'then': 55, 'after': 0, 'tried': 60, 'dispute': 20, 'much': 37, 'trouble': 61, 'bringing': 9, 'back': 4, 'up': 62}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Initialising a CountVectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Storing the first row in Text\n",
    "txt = [data[\"X\"].iloc[0]]\n",
    "\n",
    "#Printing the first row\n",
    "print (\"\\nFirst Row:\\n\",txt)\n",
    "\n",
    "#Fitting the CountVectorizer objext\n",
    "cv.fit(txt)\n",
    "\n",
    "#Transforming the first row\n",
    "vector = cv.transform(txt)\n",
    "\n",
    "\n",
    "print (\"\\nVector Shape:\\n\", vector.shape)\n",
    "\n",
    "#Storing the values of vector in array format\n",
    "vector_values = vector.toarray()\n",
    "\n",
    "print(\"\\nVector Values:\\n\",vector_values)\n",
    "\n",
    "print(\"These are the counts of the 69 unique words in our first complaint.\")\n",
    "\n",
    "print (\"\\nCount Vectorizer Vocabulary:\\n\",cv.vocabulary_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The vocabulary only specifies the index of the word and the not the counts.\n",
    "\n",
    "Comparing the vector values with vocabulary helps in identifying the word count.\n",
    "\n",
    "So for the word with index 1 is and we see its value is 5. That means the count of the word and is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 8, 1, 1, 1, 5, 3, 1, 1, 1, 1, 5, 2, 3, 3, 1]\n",
      "count value of the word at index 22\n",
      "1\n",
      "count value of the word at index 34, the word is 'loan'\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# #Converting the vector values to list\n",
    "vector_values = vector_values.tolist()[0]\n",
    "print (vector_values)\n",
    "\n",
    "\n",
    "print (\"count value of the word at index 22\")\n",
    "print (vector_values[22]) \n",
    "\n",
    "print (\"count value of the word at index 34, the word is 'loan'\")\n",
    "print (vector_values[34]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Vectorisation\n",
    "In this task we will try to implement the vectorisation on all rows and implement a logistic regression model on the vectorised dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48507462686567165\n"
     ]
    }
   ],
   "source": [
    "#Subsetting 'X'\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "#Converting 'X' to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "#Initialising a count vectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Creating the count vectorizer of our 'X' column\n",
    "vector =cv.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the count vectoriser to array\n",
    "X = vector.toarray()\n",
    "\n",
    "#Subsetting y\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "#Initialising a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model on train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score on test data\n",
    "acc = log_reg.score(X_test,y_test)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords\n",
    "In the previous task, we have seen 49% accuracy of predicting the product category. Now the question we need to ask is - can we improve the accuracy further? The answer lies in dealing with stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then', 'below', \"aren't\", 'each', 'an', 'as', 'will', 'this', 'won', 'y', 'off', 'such', 't', \"shan't\", 'weren', 'these', 'out', 'yourselves', \"don't\", 'm', \"hadn't\", 'ain', 'in', 'by', 'do', 'your', 'themselves', 'him', 'they', 'very', 'why', 'its', 'ours', \"you'd\", 'few', 'couldn', 'but', 'd', 'now', 'be', 'itself', 'once', 'other', 'it', 'i', 'or', 'again', 'there', 'between', 'when', 'she', 'only', 'own', 'too', 'should', 'while', \"couldn't\", 'ourselves', 'whom', 'doesn', 'we', \"mustn't\", 'am', 'himself', 'does', 'no', 'same', 'isn', 'shan', 'of', 'me', 'if', 'mightn', \"didn't\", \"weren't\", \"it's\", \"doesn't\", 'over', 'up', 'don', 'against', 'shouldn', 'what', 'about', 'before', 'didn', 'ma', 'those', 'theirs', \"isn't\", 'that', 'until', 'nor', \"shouldn't\", 'being', 'myself', 'aren', 's', \"you'll\", 'through', \"that'll\", 'is', 'any', \"you've\", \"you're\", 'some', 'than', 'down', 'a', 'did', \"hasn't\", 'haven', 'hers', 'at', 'most', \"she's\", 'all', 're', \"mightn't\", 'here', 'where', 'and', 'his', 'on', 'to', 'needn', 'hasn', 'yours', 'had', 'into', 'mustn', 'were', 'having', 'their', 'was', 'during', 'herself', 'doing', 'yourself', 'with', 'are', 'has', \"won't\", 'you', 'been', 'll', 'for', 'my', 'our', \"wasn't\", 'after', 'so', 'further', 'because', \"should've\", 'wouldn', 'just', 'can', 'not', 'he', \"needn't\", 'hadn', 'under', 'wasn', 'who', 'above', 'both', 'which', 'the', 'more', 'them', 'how', 'have', 've', \"haven't\", 'o', 'from', 'her', \"wouldn't\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print (set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print (list(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add our own list of stop words we want to remove from our body of text. Different domains can have different stopwords - for example, if we are classifying medical articles into different subdomains like orthopedic and neurology, then the word medicine would be a stopword for our case. So we can add medicine to the set of stopwords in the following manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "custom_set_of_stopwords = set(stopwords.words('english')+list(punctuation)+[\"medicine\"])\n",
    "print (\"medicine\" in custom_set_of_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Complaint:\n",
      " When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Bag of words of first complaint:\n",
      " ['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n",
      "\n",
      "Len of bag of words:\n",
      " 137\n",
      "\n",
      "Bag of words with stopwords removed:\n",
      " ['When', 'loan', 'switched', 'Navient', 'never', 'told', 'deliquint', 'balance', 'XXXX', 'When', 'going', 'purchase', 'vehicle', 'discovered', 'credit', 'score', 'dropped', 'XXXX', 'XXXX', 'I', 'faithful', 'paying', 'student', 'loan', 'I', 'told', 'Navient', 'company', 'delinquency', 'I', 'contacted', 'Navient', 'resolve', 'issue', 'kept', 'told', 'contact', 'credit', 'bureaus', 'expalin', 'situation', 'maybe', 'could', 'help', 'I', 'angry', 'hurried', 'paid', 'balance', 'tried', 'dispute', 'delinquency', 'credit', 'bureaus', 'I', 'much', 'trouble', 'bringing', 'credit', 'score', 'back']\n",
      "Len of bag of words with stopwords removed:\n",
      " 61\n"
     ]
    }
   ],
   "source": [
    "#Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "print(\"\\nFirst Complaint:\\n\",first_complaint)\n",
    "\n",
    "bag_of_words = word_tokenize(first_complaint)\n",
    "\n",
    "print (\"\\nBag of words of first complaint:\\n\",bag_of_words)\n",
    "print(\"\\nLen of bag of words:\\n\",len(bag_of_words))\n",
    "\n",
    "#Removing stopwords\n",
    "bow_stopwords_removed = [x for x in bag_of_words if x not in custom_set_of_stopwords]\n",
    "\n",
    "print (\"\\nBag of words with stopwords removed:\\n\",bow_stopwords_removed)\n",
    "\n",
    "print(\"Len of bag of words with stopwords removed:\\n\",len(bow_stopwords_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying in whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5373134328358209\n"
     ]
    }
   ],
   "source": [
    "#Initialising the count vectorizer with stop words parameter\n",
    "cv_stop = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Creating the count vectorizer of our 'X' column\n",
    "vector_stop = cv_stop.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the count vectoriser to array\n",
    "X_stop = vector_stop.toarray()\n",
    "\n",
    "#Splitting the data to train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_stop,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initalising a logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model on train\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score on test data\n",
    "stop_acc = log_reg.score(X_test,y_test)\n",
    "print (stop_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "In the above cell, we saw how text was converted to numerics using a count vectorizer.\n",
    "\n",
    "In other words, a count vectorizer, counts the occurences of the words in a document and all the documents are considered independent of each other. Very similar to a one hot encoding or pandas getdummies function. However in cases where multiple documents are involved, count vectorizer still does not assume any interdependence between the documents and considers each of the documents as a seperate entity.\n",
    "\n",
    "It does not rank the words based on their importance in the document, but just based on whether they exist or not. This is not a wrong approach, but it intuitively makes more sense to rank words based on their importance in the document right? In fact, the process of converting, text to numbers should essentially be a ranking system of the words so that the documents can each get a score based on what words they contain. All words cannot have the same imprtance or relevance in the document right?\n",
    "\n",
    "There are two ways to approach document similarity:\n",
    "\n",
    "TF-IDF Score\n",
    "\n",
    "Cosine Similarity\n",
    "\n",
    "Let's look at them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF!!\n",
    "TF-IDF or Term Frequency and Inverse Document Frequency is kind of the holy grail of ranking metrics to convert text to numbers. Consider the count vectorizer as a metric which just counts the occurences of words in a document.\n",
    "\n",
    "TF-IDF takes it a step further and ranks the words based not just on their occurences in one document but across all the documents. Hence if CV or Count vectorizer was giving more importance to words because they have appeared multiple times in the document, TF-IDF will rank them high if they have appeared only in that document, meaning that they are rare, hence higher importance and lower if they have appeared in all or most documents, because they are more common, hence lower ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "Example\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint 1:  When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Complaint 2:  I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them\n",
      "\n",
      "Complaint 3:  My mortgage is with BB & T Bank, recently I have been investigating ways to pay down my mortgage faster and I came across Biweekly Mortgage Calculator on BB & T 's website. It's a nice, easy to use calculator that you plug in your interest rate, mortgage amount, mortgage term, and payment type and it calculates your accelerated bi-weekly payment for you and shows you how much quicker you can pay down your loan. Ours figured out to pay off a 30 year mortgage in 26.4 years ... quite a savings! \n",
      "I called BB & T 's customer service number to inquire how I get set up on this payment plan. I was told they do not offer that type of payment plan, but I could send in my payments bi-weekly but it would not be applied until the full amount was received. ( the money would sit in a \" holding account '' until the full payment amount was collected ). I ended up calling back a few days later thinking the rep I was talking to didn't understand what I wanted to do or was not knowledgeable of this program. I got the SAME ANSWER! \n",
      "I then asked for the corporate BB & T office number where I could speak to someone that was knowledgeable of this product. After 3 days I received a phone call back from a corporate manager stating they do not offer this product, and they were \" checking into why this is on their website ''. She stated they do have a few customers that make bi-weekly payments, but they no longer offer this service. \n",
      "I don't understand how they can have this active link on their website under their Financial Planning Center tab to mislead customers when all they say is \" I'm sorry, I know you're upset about this '' Sounds like false advertising to me! \n",
      "https : //www.bbt.com/XXXX\n"
     ]
    }
   ],
   "source": [
    "complaint_1 = data[\"X\"].iloc[0]\n",
    "complaint_2 = data[\"X\"].iloc[1]\n",
    "complaint_3 = data[\"X\"].iloc[2]\n",
    "\n",
    "print (\"Complaint 1: \", complaint_1)\n",
    "\n",
    "print (\"\\nComplaint 2: \", complaint_2)\n",
    "\n",
    "print (\"\\nComplaint 3: \", complaint_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized sentence: (3, 214)\n",
      "The tf-idf score of first five elements: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dictionary of words with tf-idf score:\n",
      " {'26': 0.0, '30': 0.0, 'about': 0.0, 'accelerated': 0.0, 'access': 0.0, 'account': 0.0, 'across': 0.0, 'active': 0.0, 'advertising': 0.0, 'after': 0.05253580411952334, 'all': 0.0, 'amount': 0.0, 'and': 0.20399369240069398, 'angry': 0.06907826902804956, 'answer': 0.0, 'applied': 0.0, 'asked': 0.0, 'at': 0.06907826902804956, 'back': 0.05253580411952334, 'balance': 0.13815653805609912, 'bank': 0.0, 'bb': 0.0, 'bbt': 0.0, 'be': 0.0, 'because': 0.06907826902804956, 'been': 0.10507160823904668, 'being': 0.06907826902804956, 'bi': 0.0, 'biweekly': 0.0, 'bringing': 0.06907826902804956, 'bureaus': 0.13815653805609912, 'but': 0.0, 'calculates': 0.0, 'calculator': 0.0, 'call': 0.0, 'called': 0.0, 'calling': 0.0, 'came': 0.0, 'can': 0.0, 'capital': 0.0, 'center': 0.0, 'checking': 0.0, 'collected': 0.0, 'com': 0.0, 'company': 0.06907826902804956, 'contact': 0.06907826902804956, 'contacted': 0.06907826902804956, 'corporate': 0.0, 'could': 0.05253580411952334, 'credit': 0.27631307611219824, 'customer': 0.0, 'customers': 0.0, 'days': 0.0, 'delinquency': 0.13815653805609912, 'deliquint': 0.06907826902804956, 'did': 0.06907826902804956, 'didn': 0.0, 'discovered': 0.06907826902804956, 'dispute': 0.06907826902804956, 'do': 0.0, 'don': 0.0, 'down': 0.0, 'dropped': 0.06907826902804956, 'easy': 0.0, 'ended': 0.0, 'expalin': 0.06907826902804956, 'faithful': 0.06907826902804956, 'false': 0.0, 'faster': 0.0, 'few': 0.0, 'figured': 0.0, 'financial': 0.0, 'for': 0.0, 'from': 0.05253580411952334, 'full': 0.0, 'get': 0.0, 'going': 0.06907826902804956, 'got': 0.0, 'had': 0.27631307611219824, 'have': 0.10507160823904668, 'help': 0.06907826902804956, 'holding': 0.0, 'how': 0.0, 'https': 0.0, 'hurried': 0.06907826902804956, 'in': 0.0, 'inquire': 0.0, 'interest': 0.0, 'into': 0.05253580411952334, 'investigating': 0.0, 'is': 0.0, 'issue': 0.06907826902804956, 'it': 0.0, 'just': 0.13815653805609912, 'kept': 0.06907826902804956, 'know': 0.0, 'knowledgeable': 0.0, 'later': 0.0, 'let': 0.0, 'like': 0.0, 'link': 0.0, 'loan': 0.10507160823904668, 'longer': 0.0, 'make': 0.0, 'manager': 0.0, 'maybe': 0.06907826902804956, 'me': 0.0407987384801388, 'mislead': 0.0, 'money': 0.0, 'monitoring': 0.0, 'mortgage': 0.0, 'much': 0.05253580411952334, 'my': 0.1631949539205552, 'navient': 0.20723480708414868, 'never': 0.06907826902804956, 'nice': 0.0, 'no': 0.0, 'not': 0.0407987384801388, 'number': 0.0, 'of': 0.0, 'off': 0.05253580411952334, 'offer': 0.0, 'office': 0.0, 'on': 0.0, 'one': 0.0, 'or': 0.0, 'ours': 0.0, 'out': 0.0, 'over': 0.06907826902804956, 'paid': 0.06907826902804956, 'pay': 0.0, 'paying': 0.06907826902804956, 'payment': 0.0, 'payments': 0.0, 'phone': 0.0, 'plan': 0.0, 'planning': 0.0, 'plug': 0.0, 'product': 0.0, 'program': 0.0, 'purchase': 0.06907826902804956, 'quicker': 0.0, 'quite': 0.0, 'rate': 0.0, 're': 0.0, 'received': 0.0, 'recently': 0.0, 'rep': 0.0, 'resolve': 0.06907826902804956, 'same': 0.0, 'savings': 0.0, 'say': 0.0, 'score': 0.13815653805609912, 'send': 0.0, 'service': 0.0, 'set': 0.0, 'she': 0.0, 'shows': 0.0, 'sign': 0.0, 'sit': 0.0, 'situation': 0.06907826902804956, 'so': 0.13815653805609912, 'someone': 0.0, 'sorry': 0.0, 'sounds': 0.0, 'speak': 0.0, 'spending': 0.0, 'stated': 0.0, 'stating': 0.0, 'student': 0.06907826902804956, 'switched': 0.06907826902804956, 'tab': 0.0, 'talking': 0.0, 'term': 0.0, 'that': 0.15760741235857004, 'the': 0.42028643295618673, 'their': 0.0, 'them': 0.0, 'then': 0.05253580411952334, 'they': 0.05253580411952334, 'thinking': 0.0, 'this': 0.05253580411952334, 'through': 0.0, 'to': 0.20399369240069398, 'told': 0.15760741235857004, 'tried': 0.05253580411952334, 'trouble': 0.06907826902804956, 'type': 0.0, 'under': 0.0, 'understand': 0.0, 'until': 0.0, 'up': 0.0407987384801388, 'upset': 0.0, 'use': 0.0, 'vehicle': 0.06907826902804956, 'wanted': 0.0, 'was': 0.2626790205976167, 'ways': 0.0, 'website': 0.0, 'weekly': 0.0, 'were': 0.0, 'what': 0.0, 'when': 0.10507160823904668, 'where': 0.0, 'why': 0.0, 'will': 0.0, 'with': 0.15760741235857004, 'would': 0.0, 'www': 0.0, 'xxxx': 0.15760741235857004, 'year': 0.0, 'years': 0.0, 'you': 0.05253580411952334, 'your': 0.0}\n",
      "Sorted dictonary:\n",
      "\n",
      "[('the', 0.42028643295618673), ('credit', 0.27631307611219824), ('had', 0.27631307611219824), ('was', 0.2626790205976167), ('navient', 0.20723480708414868), ('and', 0.20399369240069398), ('to', 0.20399369240069398), ('my', 0.1631949539205552), ('that', 0.15760741235857004), ('told', 0.15760741235857004), ('with', 0.15760741235857004), ('xxxx', 0.15760741235857004), ('balance', 0.13815653805609912), ('bureaus', 0.13815653805609912), ('delinquency', 0.13815653805609912), ('just', 0.13815653805609912), ('score', 0.13815653805609912), ('so', 0.13815653805609912), ('been', 0.10507160823904668), ('have', 0.10507160823904668), ('loan', 0.10507160823904668), ('when', 0.10507160823904668), ('angry', 0.06907826902804956), ('at', 0.06907826902804956), ('because', 0.06907826902804956), ('being', 0.06907826902804956), ('bringing', 0.06907826902804956), ('company', 0.06907826902804956), ('contact', 0.06907826902804956), ('contacted', 0.06907826902804956), ('deliquint', 0.06907826902804956), ('did', 0.06907826902804956), ('discovered', 0.06907826902804956), ('dispute', 0.06907826902804956), ('dropped', 0.06907826902804956), ('expalin', 0.06907826902804956), ('faithful', 0.06907826902804956), ('going', 0.06907826902804956), ('help', 0.06907826902804956), ('hurried', 0.06907826902804956), ('issue', 0.06907826902804956), ('kept', 0.06907826902804956), ('maybe', 0.06907826902804956), ('never', 0.06907826902804956), ('over', 0.06907826902804956), ('paid', 0.06907826902804956), ('paying', 0.06907826902804956), ('purchase', 0.06907826902804956), ('resolve', 0.06907826902804956), ('situation', 0.06907826902804956), ('student', 0.06907826902804956), ('switched', 0.06907826902804956), ('trouble', 0.06907826902804956), ('vehicle', 0.06907826902804956), ('after', 0.05253580411952334), ('back', 0.05253580411952334), ('could', 0.05253580411952334), ('from', 0.05253580411952334), ('into', 0.05253580411952334), ('much', 0.05253580411952334), ('off', 0.05253580411952334), ('then', 0.05253580411952334), ('they', 0.05253580411952334), ('this', 0.05253580411952334), ('tried', 0.05253580411952334), ('you', 0.05253580411952334), ('me', 0.0407987384801388), ('not', 0.0407987384801388), ('up', 0.0407987384801388), ('26', 0.0), ('30', 0.0), ('about', 0.0), ('accelerated', 0.0), ('access', 0.0), ('account', 0.0), ('across', 0.0), ('active', 0.0), ('advertising', 0.0), ('all', 0.0), ('amount', 0.0), ('answer', 0.0), ('applied', 0.0), ('asked', 0.0), ('bank', 0.0), ('bb', 0.0), ('bbt', 0.0), ('be', 0.0), ('bi', 0.0), ('biweekly', 0.0), ('but', 0.0), ('calculates', 0.0), ('calculator', 0.0), ('call', 0.0), ('called', 0.0), ('calling', 0.0), ('came', 0.0), ('can', 0.0), ('capital', 0.0), ('center', 0.0), ('checking', 0.0), ('collected', 0.0), ('com', 0.0), ('corporate', 0.0), ('customer', 0.0), ('customers', 0.0), ('days', 0.0), ('didn', 0.0), ('do', 0.0), ('don', 0.0), ('down', 0.0), ('easy', 0.0), ('ended', 0.0), ('false', 0.0), ('faster', 0.0), ('few', 0.0), ('figured', 0.0), ('financial', 0.0), ('for', 0.0), ('full', 0.0), ('get', 0.0), ('got', 0.0), ('holding', 0.0), ('how', 0.0), ('https', 0.0), ('in', 0.0), ('inquire', 0.0), ('interest', 0.0), ('investigating', 0.0), ('is', 0.0), ('it', 0.0), ('know', 0.0), ('knowledgeable', 0.0), ('later', 0.0), ('let', 0.0), ('like', 0.0), ('link', 0.0), ('longer', 0.0), ('make', 0.0), ('manager', 0.0), ('mislead', 0.0), ('money', 0.0), ('monitoring', 0.0), ('mortgage', 0.0), ('nice', 0.0), ('no', 0.0), ('number', 0.0), ('of', 0.0), ('offer', 0.0), ('office', 0.0), ('on', 0.0), ('one', 0.0), ('or', 0.0), ('ours', 0.0), ('out', 0.0), ('pay', 0.0), ('payment', 0.0), ('payments', 0.0), ('phone', 0.0), ('plan', 0.0), ('planning', 0.0), ('plug', 0.0), ('product', 0.0), ('program', 0.0), ('quicker', 0.0), ('quite', 0.0), ('rate', 0.0), ('re', 0.0), ('received', 0.0), ('recently', 0.0), ('rep', 0.0), ('same', 0.0), ('savings', 0.0), ('say', 0.0), ('send', 0.0), ('service', 0.0), ('set', 0.0), ('she', 0.0), ('shows', 0.0), ('sign', 0.0), ('sit', 0.0), ('someone', 0.0), ('sorry', 0.0), ('sounds', 0.0), ('speak', 0.0), ('spending', 0.0), ('stated', 0.0), ('stating', 0.0), ('tab', 0.0), ('talking', 0.0), ('term', 0.0), ('their', 0.0), ('them', 0.0), ('thinking', 0.0), ('through', 0.0), ('type', 0.0), ('under', 0.0), ('understand', 0.0), ('until', 0.0), ('upset', 0.0), ('use', 0.0), ('wanted', 0.0), ('ways', 0.0), ('website', 0.0), ('weekly', 0.0), ('were', 0.0), ('what', 0.0), ('where', 0.0), ('why', 0.0), ('will', 0.0), ('would', 0.0), ('www', 0.0), ('year', 0.0), ('years', 0.0), ('your', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "sents = [complaint_1, complaint_2, complaint_3]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "\n",
    "vectorizer.fit(sents)\n",
    "\n",
    "vector = vectorizer.transform(sents)\n",
    "\n",
    "print(\"Shape of the vectorized sentence:\",vector.shape)\n",
    "\n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "\n",
    "print(\"The tf-idf score of first five elements:\",vector_values[:5])\n",
    "\n",
    "\n",
    "# Converting the tf-idf score with the word into a dictionary\n",
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "\n",
    "print(\"Dictionary of words with tf-idf score:\\n\", d)\n",
    "\n",
    "#Sorting this dictionary by value in the descending order to see the ranking\n",
    "print(\"Sorted dictonary:\\n\")\n",
    "print (sorted(d.items(), key=operator.itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the model learns to give lesser importance to words like is,it,in etc;. Unfortunately, it also gives a low importance to important words like financial, mortgage and a fairly high importance to unwanted words like the, was. It does give higher importance to words such as credit. And that is because TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model. With a larger corpus, these issues would be resolved when a lot more documents would have words like financial but not the.\n",
    "\n",
    "Rerunning this for about 100 documents, we see that the ranking is completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted dictionary: \n",
      "\n",
      "[('navient', 0.35431369455512246), ('the', 0.24041090745878946), ('delinquency', 0.23620912970341496), ('had', 0.20963581996093744), ('told', 0.19801036084460227), ('bureaus', 0.19189624902422267), ('was', 0.18441012909485335), ('score', 0.1637072393027124), ('credit', 0.15907030235158917), ('just', 0.15203704157649714), ('balance', 0.14549112699503114), ('to', 0.14437952896171652), ('and', 0.14013869297167467), ('loan', 0.1344398602659683), ('angry', 0.12870728663065903), ('deliquint', 0.12870728663065903), ('expalin', 0.12870728663065903), ('faithful', 0.12870728663065903), ('hurried', 0.12870728663065903), ('switched', 0.12870728663065903)]\n"
     ]
    }
   ],
   "source": [
    "sents=[]\n",
    "for x in range(100):\n",
    "    sents.append(data[\"X\"].iloc[x])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "vector.shape \n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print(\"Sorted dictionary: \\n\")\n",
    "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can notice that \"the\" has moved down from 0.42 to 0.27. Navient has increased from 0.20 to 0.35. So has bureaus from 0.13 to 0.19. As we include more and more sentences, the words whch have appeared more and more frequently across all the documents, such as \"the\" are moving down in value, and words like bureau and navient, which have appeared far lesser number of times have started increasing. Which reiterates the point we had. TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44029850746268656\n"
     ]
    }
   ],
   "source": [
    "#Initialising the tf-idf model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Vectorizing the 'X' column\n",
    "vector =tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model with train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "tfidf_acc = log_reg.score(X_test,y_test)\n",
    "print (tfidf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes Classifier\n",
    "Naive Bayes classifier is a linear classifier based on the Bayes' theorem. The term naive comes from the assumption of considering all features in a dataset are mutually independent. The independent assumption is generally violated in real datasets, but the naive Bayes Classifier still tends to perform very well.\n",
    "\n",
    "In this task instead of our normal dataset, we will be using a larger sized dataset having the same features but 10000 rows. After loading it from a csv file, we will apply the TF-IDF vectorization and then implement the Naive-Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42461964038727523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# reading the data\n",
    "data = pd.read_csv('file2.csv')\n",
    "\n",
    "# keeping the relevant columns\n",
    "data = data[[\"Consumer complaint narrative\", \"Product\"]]\n",
    "\n",
    "# renaming the columns\n",
    "data.columns = [\"X\", \"y\"]\n",
    "\n",
    "# dropping the nan values\n",
    "data = data.dropna()\n",
    "\n",
    "# X\n",
    "\n",
    "# Subsetting 'X' column\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "# Converting the 'X' column to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "# Initialising a tfidf vectorizer object with stopwords\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Vectorizing the 'X' column\n",
    "vector = tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "# Converting vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "# y\n",
    "\n",
    "# Subsetting 'y' column\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "# Initialising label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, labels[\"y\"], test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialsing a naive bayes classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fitting the model on train data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Finding the accuracy score of model on test data\n",
    "nb_acc = nb.score(X_test, y_test)\n",
    "print(nb_acc)\n",
    "\n",
    "#Code ends here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016597510373444\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Code starts here\n",
    "#Initialising a random over sampler object\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "#Sampling the train data\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "#Initialsing multinomial naive bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Fitting the sampled train data\n",
    "nb.fit(X_ros,y_ros)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "ros_score=nb.score(X_test,y_test)\n",
    "print(ros_score)\n",
    "\n",
    "#Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SVM\n",
    "Support Vector Machines are based on the concept of decision planes that define decision boundaries. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which can help categorize new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why SVMs work for text classification?\n",
    "\n",
    "High Dimensional input space:\n",
    "When dealing with text data, we know we need to deal with many features(>10000 usually). Since SVM(particulary Linear SVM) uses overfitting protection, they have the capability to handle large feature space.\n",
    "\n",
    "Few irrelevant features:\n",
    "Extension of the above point, during text classification one can't really do a rigourous feature selection. Research has shown that even the features ranked low still contain considerable information. SVM is therefore apt to handle this large amount of feature space in which feature selection or reduction can't be achieved satisfactorily.\n",
    "\n",
    "Most text categorisation problems are linearly separable\n",
    "Lot of experiments has resulted in the conclusion that text categorisation problems are usually linearly separable, since the concept of SVM is to find such linear separators, SVMS work better than most other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between ML text classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "Naive Bayes\n",
    "\n",
    "- Performs well in while dealing with small amount training data(spam filtering and email categorization) to estimate the parameters for classification\n",
    "\n",
    "- Works well on numeric and textual data and easy to implement\n",
    "\n",
    "SVM\n",
    "\n",
    "- Captures the inherent characteristics of the data better and handles missing data well\n",
    "\n",
    "-When you are building the model from the features point of view , SVM looks at the interaction between the features to a certain degree.\n",
    "\n",
    "- SVM handle efiiceiently Presence of very few irrelevant features ,Linear separability of data\n",
    "\n",
    "<h3>Disadvantages</h3>\n",
    "\n",
    "Naive Bayes\n",
    "\n",
    "- Performs very poorly when features are highly correlated and does not the consider frequency of word occurrences\n",
    "\n",
    "SVM\n",
    "\n",
    "- Difficult to do parameter tuning and kernel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648686030428769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Code starts here\n",
    "\n",
    "svc = SVC(random_state = 0, kernel = 'linear')\n",
    "svc.fit(X_ros,y_ros)\n",
    "\n",
    "print(svc.score(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
